# parallel
训练上主要是分为几种并行，首先是在单个gpu上训练，
其次是多gpu上训练，最后是分布式训练(multi-machine)

## base
指定设备
```python
import torch
device = torch.device("cuda:0")
```
获取所有设备
```python
import torch
no_cuda = False
device = torch.device("cuda" if torch.cuda.is_available() and not no_cuda else "cpu")

```
获取设备数量
```python
import torch
n_gpu = torch.cuda.device_count()
```

## single gpu
需要做两步
- 将模型发放到gpu上
```python
import torch
device = torch.device("cuda:0")
model.to(device)

```
- 将数据也发放到gpu上
```python
import torch
device = torch.device("cuda:0")
mytensor = my_tensor.to(device)
```

## multi-gpu
需要额外的做的一步，将model wrap一下，这个操作会
将输入的数据按batch维度划分，假设batch_size=32，若有两个gpu，
则每个gpu上运行batch_size=16的数据
```python
import torch
if n_gpu>1:
    model = torch.nn.DataParallel(model)
model.to(device)
```
若是多gpu，最好是平均一下计算所得的loss再backward
```python
loss=model(input)
loss = loss.mean()
loss.backward()
```

## distributed

# 关于TriLinear的实现
对于$\alpha (h, u) = w^{T}[h;u;h \circ u]$，需要将$w$分为三个部分$[w_1,w_2,w_3]$，
分别计算三个$s$，再将结果相加，这样能够省内存，
其中针对$h$和$u$，需要利用expand扩展，
而$h\circ u$则需要利用以下公式计算：
$$
s_3=w^{T}_{3}(h\circ u) = (w_{3} \circ h)^{T}u
$$
其中对于batch计算要更复杂些,具体可以参考实现
[TriLinear](../mrc/nn/similarity_function.py)。


# 增加项目的测试单元
如何添加单元测试呢，这里使用unittest模块来实现，继承unittest.TestCase
，简单的测试一些函数或者类是否符合标准，比如在[Test](../tests/nn/similarity_function_test.py)
中对TriLinear类进行简单的测试，在setUp中初始化输入和TriLinear类，接着进行单元测试。
添加这些测试能够初步保证代码能够跑通。